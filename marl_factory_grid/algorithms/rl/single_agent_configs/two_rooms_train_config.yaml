env:
  classname:          marl_factory_grid.environment.configs.rl
  n_agents:           1 # Number of agents in the environment
  train_render:       False # If training should be graphically visualized
  save_and_log:       True # If configurations and potential logging files should be saved
algorithm:
  gamma:              0.99 # The gamma value that is used as discounting factor
  n_steps:            0 # How much experience should be sampled at most until the next value- and policy-net updates are performed. (0 = Monte Carlo)
  chunk-episode:      20000 # For update, splits very large episodes in batches of approximately equal size. (0 = update networks with full episode at once)
  max_steps:          260000 # Number of training steps used to train the agent. Here, only a placeholder value
  advantage:          "Advantage-AC" # Defines the used actor critic model
  pile-order:         "fixed" # Clean dirt piles (=encoded flags) in a fixed order specified by the environment config (cf. coords_or_quantity)
  pile-observability: "single" # Agent can only perceive one dirt pile at any given time step
  pile_all_done:      "single" # Episode ends when the current target pile is cleaned
  auxiliary_piles:    False # Auxiliary piles are only differentiated from regular target piles during marl eval


