env:
  classname:          marl_factory_grid.environment.configs.rl
  env_name:           "rl/dirt_quadrant_agent1_train_config"
  n_agents:           1 # Number of agents in the environment
  train_render:       False # If training should be graphically visualized
  save_and_log:       True # If configurations and potential logging files should be saved
algorithm:
  gamma:              0.99 # The gamma value that is used as discounting factor
  n_steps:            0 # How much experience should be sampled at most until the next value- and policy-net updates are performed. (0 = Monte Carlo)
  chunk-episode:      20000 # For update, splits very large episodes in batches of approximately equal size. (0 = update networks with full episode at once)
  max_steps:          140000 # Number of training steps used for agent1 (=agent2)
  advantage:          "Advantage-AC" # Defines the used actor critic model
  pile-order:         "fixed" # Clean dirt piles in a fixed order specified by the environment config (cf. coords_or_quantity)
  pile-observability: "single" # Agent can only perceive one dirt pile at any given time step
  pile_all_done:      "single" # Episode ends when the current target pile is cleaned
  auxiliary_piles:    False # Dirt quadrant does not use this option

